{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fcf120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: geopandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (1.3.4)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (3.3.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (1.8.21)\n",
      "Requirement already satisfied: shapely<2,>=1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (1.8.2)\n",
      "Requirement already satisfied: six>=1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: click>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (8.0.3)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: munch in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (59.4.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.0.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.0.0->geopandas) (1.20.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging->geopandas) (3.0.6)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: geopy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopy) (1.52)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: shapely in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (1.8.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pandasql in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (0.7.3)\n",
      "Requirement already satisfied: sqlalchemy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandasql) (1.4.27)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandasql) (1.3.4)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandasql) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->pandasql) (2021.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sqlalchemy->pandasql) (1.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Install libraries\n",
    "!pip install geopandas\n",
    "!pip install geopy\n",
    "!pip install shapely \n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551a8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import geopandas as ps\n",
    "import geopy as gy\n",
    "import shapely as sy\n",
    "import dask.dataframe as dd\n",
    "import pandasql as pq\n",
    "from shapely.geometry import Point, Polygon\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11d85e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total record count :  3303988\n"
     ]
    }
   ],
   "source": [
    "#Setup the path for the file -- might have to change this if yours is named differently\n",
    "bucket = 'daen-690-pacific-deviations/raw-data' #Bucket name\n",
    "data_key = 'TOMRDate=2021-12-24.csv' #Path to the CSV file \n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "#Import all of the raw data \n",
    "rawData_df = dd.read_csv(data_location, assume_missing=True)\n",
    "print(f'Total record count : ',len(rawData_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0413bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAttributes():\n",
    "    #New dataframe with selected attributes from the raw data\n",
    "    airspaceData_df = rawData_df[[\"FRN73TMRPDateTimeOfMessageRec\",\"FRN131HRPWCFloatingPointLat\",\"FRN131HRPWCFloatingPointLong\",\n",
    "                     \"FRN145FLFlightLevel\", \"FRN170TITargetId\",\"RESHSelectedHeading\",\"FRN80TATargetAddress\",\n",
    "                     \"FRN161TNTrackNumber\"]]\n",
    "\n",
    "    #Rename columns to make it easier to read\n",
    "    airspaceData_df = airspaceData_df.rename(columns={'FRN73TMRPDateTimeOfMessageRec': 'DateTime', \n",
    "                                                      'FRN131HRPWCFloatingPointLat': \"Latitude\", \n",
    "                                                      'FRN131HRPWCFloatingPointLong': \"Longitude\", \n",
    "                                                      'FRN145FLFlightLevel': \"FlightLevel\", \n",
    "                                                      'FRN170TITargetId': \"TargetID\", \n",
    "                                                      'RESHSelectedHeading': \"SelectedHeading\", \n",
    "                                                      'FRN80TATargetAddress': \"TargetAddress\",\n",
    "                                                      'FRN161TNTrackNumber': \"TrackNumber\"})\n",
    "    \n",
    "    # Remove anything less than 240 flight level \n",
    "    airspaceData_df = airspaceData_df[(airspaceData_df['FlightLevel'] >= 240)]\n",
    "    \n",
    "    #Change flight level scale to feet (FL1 = 100 ft)\n",
    "    airspaceData_df['FlightLevel'] = airspaceData_df['FlightLevel'].apply(lambda x: x * 100, meta=('FlightLevel', 'float64'))\n",
    "    \n",
    "    airspaceData = airspaceData_df.compute()\n",
    "    \n",
    "    return airspaceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d34cfa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstFiveSeconds():\n",
    "    \n",
    "    #Set the dataframe that will be altered through this block of code\n",
    "    global airspaceData\n",
    "    \n",
    "    char = ['T','Z']\n",
    "    for x in char:\n",
    "        airspaceData[\"DateTime\"] = airspaceData[\"DateTime\"].str.replace( x ,\" \")\n",
    "\n",
    "    # Formatted Datetime\n",
    "    airspaceData[\"DateTime\"] = pd.to_datetime(airspaceData[\"DateTime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create 4 new columns for Hour, Minute, Second and Microsecond\n",
    "    airspaceData[\"Hour\"] = airspaceData[\"DateTime\"].dt.hour\n",
    "    airspaceData[\"Minute\"] = airspaceData[\"DateTime\"].dt.minute\n",
    "    airspaceData[\"Second\"] = airspaceData[\"DateTime\"].dt.second\n",
    "    airspaceData[\"microSecond\"] = airspaceData[\"DateTime\"].dt.microsecond\n",
    "    \n",
    "    # Reorder columns\n",
    "    airspaceData = airspaceData[[\"DateTime\",\"Hour\",\"Minute\",\"Second\",\"microSecond\",\"Latitude\",\"Longitude\",\"FlightLevel\",\n",
    "                                   \"TargetID\",\"SelectedHeading\",\"TargetAddress\",\n",
    "                                   \"TrackNumber\"]]\n",
    "    \n",
    "    rawAircraftData = airspaceData\n",
    "    \n",
    "    #Keep only records for the first 5 seconds to speed up processing time \n",
    "    airspaceData = airspaceData[(airspaceData['Second'] < 5)]\n",
    "    \n",
    "    return (rawAircraftData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d897fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHISpace():\n",
    "    \n",
    "    #Set the dataframe that will be altered through this block of code\n",
    "    global airspaceData\n",
    "    \n",
    "    #Coordinates for Hawaii airspace\n",
    "    v0 = (26.14472222, -158.62194444) \n",
    "    v1 = (26.105, -160.63166667)\n",
    "    v2 = (25.67611111, -161.69111111)\n",
    "    v3 = (25.05666667, -162.64972222)\n",
    "    v4 = (24.16889, -163.26638889)\n",
    "    v5 = (23.25833, -163.855)\n",
    "    v6 = (22.20555556, -163.91444444)\n",
    "    v7 = (33.10266389, 130.47177778)\n",
    "    v8 = (20.11666667, -163.3)\n",
    "    v9 = (19.65805556,-162.69944444)\n",
    "    v10 = (19.415, -162.38361111)\n",
    "    v11 = (18.40777778, -160.81416667)\n",
    "    v12 = (18.0525, -160.26972222)\n",
    "    v13 = (17.75583333, -159.53888889)\n",
    "    v14 = (17.17055556, -157.75666667) \n",
    "    v15 = (17.805,-156.06805556)\n",
    "    v16 = (18.10888889, -155.71166667)\n",
    "    v17 = (19.14222222, -154.48333333)\n",
    "    v18 = (19.22293333, -151.87963333)\n",
    "    v19 = (20.69694444, -151.01916667) \n",
    "    v20 = (21.54777778, -151.46638889)\n",
    "    v21 = (22.34416667,-151.88527778)\n",
    "    v22 = (23.02416667, -152.57777778)\n",
    "    v23 = (23.78055556, -153.36611111)\n",
    "    v24 = (24.29583333, -154.25)\n",
    "    v25 = (24.72138889, -155.26305556)\n",
    "    v26 = (25.19583333, -156.42111111)\n",
    "\n",
    "    # Polygon\n",
    "    coords = [v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26]\n",
    "    poly = Polygon(coords)\n",
    "    \n",
    "    #Sort flights into what is in the airspace and what is not\n",
    "    hawaiiAir = []\n",
    "    loc = 0\n",
    "\n",
    "    while loc < len(airspaceData):\n",
    "      p1 = Point(airspaceData.iloc[loc][5], airspaceData.iloc[loc][6])\n",
    "      hawaiiAir.append(p1.within(poly))\n",
    "      loc = loc + 1\n",
    "\n",
    "    airspaceData['nearHawaii'] = hawaiiAir\n",
    "    \n",
    "    #Filter out only the ones in the airspace\n",
    "    airspaceData = airspaceData[(airspaceData['nearHawaii'] == False)]\n",
    "    airspaceData = airspaceData.drop(columns=['nearHawaii'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d425953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aircraftDirection():\n",
    "    #Set the dataframe that will be altered through this block of code\n",
    "    global airspaceData\n",
    "    \n",
    "    # Replace missing value with -1\n",
    "    airspaceData['SelectedHeading'] = airspaceData['SelectedHeading'].fillna(-1)\n",
    "    \n",
    "    # Assign Direction \"E\" for 0-180 degree, \"W\" for 180-360 degree, \"NA\" is record with null values \n",
    "    conditionlist = [\n",
    "        (airspaceData['SelectedHeading'] < 0) ,\n",
    "        (airspaceData['SelectedHeading'] >= 0) & (airspaceData['SelectedHeading'] <180),\n",
    "        (airspaceData['SelectedHeading'] > 180)]\n",
    "    choicelist = ['NA', 'E', 'W']\n",
    "    airspaceData['Direction'] = np.select(conditionlist, choicelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670fe388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData():\n",
    "    \n",
    "    rawAircraftData = firstFiveSeconds()\n",
    "    removeHISpace()\n",
    "    aircraftDirection()\n",
    "    \n",
    "    return rawAircraftData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83776439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28813/88026422.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  airspaceData['nearHawaii'] = hawaiiAir\n"
     ]
    }
   ],
   "source": [
    "airspaceData = filterAttributes()\n",
    "allAircraftData = filterData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef8788b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minuteFilter(HourCounter,MinuteCounter):\n",
    "\n",
    "    global airspaceData\n",
    "\n",
    "    #create SQL query for flights between the start and end time\n",
    "    sql1 = \"SELECT *, min(Second) FROM airspaceData WHERE Hour = '{0}' and Minute = '{1}' GROUP BY TargetID ORDER BY TargetID, Second\".format(HourCounter, MinuteCounter)\n",
    "\n",
    "    #Run query and store results\n",
    "    recordsInMinute = pq.sqldf(sql1, globals())\n",
    "    del recordsInMinute['min(Second)']\n",
    "\n",
    "    return (recordsInMinute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89813bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lonTableCreate(recordsByMinuteDF):\n",
    "    #Create tables to check for proximity in latitude\n",
    "    sql4 = \"SELECT * FROM recordsByMinuteDF ORDER BY Longitude\"\n",
    "\n",
    "    #Run query and store results\n",
    "    LongitudeOrderDF = pq.sqldf(sql4, locals())\n",
    "\n",
    "    return (LongitudeOrderDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ad2ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latTableCreate(recordsByMinuteDF):\n",
    "    #Create tables to check for proximity in latitude\n",
    "    sql3 = \"SELECT * FROM recordsByMinuteDF ORDER BY Latitude\"\n",
    "\n",
    "    #Run query and store results\n",
    "    LatitudeOrderDF = pq.sqldf(sql3, locals())\n",
    "\n",
    "    return (LatitudeOrderDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9c3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the formula below\n",
    "def distance_d(point0,pointX):\n",
    "    # The function \"radians\" is found in the math module\n",
    "    LoA = radians(point0[1])  \n",
    "    LoB = radians(pointX[1])\n",
    "    LaA=  radians(point0[0])  \n",
    "    LaB = radians(pointX[0]) \n",
    "    # The \"Haversine formula\" is used.\n",
    "    D_Lo = LoB - LoA \n",
    "    D_La = LaB - LaA \n",
    "    P = sin(D_La / 2)**2 + cos(LaA) * cos(LaB) * sin(D_Lo / 2)**2  \n",
    "   \n",
    "    Q = 2 * asin(sqrt(P))   \n",
    "    # The earth's radius in kilometers.\n",
    "    R_km = 6371  \n",
    " \n",
    "    # Change the kilometer to  nautical miles\n",
    "    R_nm = R_km*0.539956803\n",
    "\n",
    "    # Then we'll compute the outcome.\n",
    "    return(Q * R_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc2b5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to set up boundary within 25 nm by latitude, longitude \n",
    "def limit_lon(point0):\n",
    "    '''\n",
    "    use with LongitudeOrderDF\n",
    "    note: distance from point to longitude boundary of each row is around 24.9715\n",
    "    '''\n",
    "    LaA = radians(point0[0])\n",
    "    onedeg_long = cos(LaA)*(69.172*0.868976242)\n",
    "    add = 25/onedeg_long \n",
    "    pointlimit = (point0[0],point0[1]+add)\n",
    "    return pointlimit[1]\n",
    "\n",
    "def limit_lat(point0):\n",
    "    '''\n",
    "    use with LatitudeOrderDF\n",
    "    note: distance from point to longitude boundary of each row is 25.016857125339488\n",
    "    '''\n",
    "    onedeg_lat = 60 \n",
    "    add = 25/onedeg_lat\n",
    "    pointlimit = (point0[0]+add,point0[1])\n",
    "    return pointlimit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "956f53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to select, merge and add the values from analyzing Longitude and Latitude\n",
    "def newDF(OrderDF,x,y,d):\n",
    "    \"\"\"DF is Long/LatitudeOrderDF\n",
    "       x = long/latpoint_a\n",
    "       y = long/latpoint_b\n",
    "       d = long/latdistance_ab\"\"\"\n",
    "    # select rows that index is in list 'point_a', 'point_b'\n",
    "    A = OrderDF.loc[x,['DateTime','Hour','Minute','Second','microSecond','Latitude','Longitude','FlightLevel',\n",
    "                             'TargetID', 'SelectedHeading', 'TargetAddress','Direction']]\n",
    "    B = OrderDF.loc[y,['DateTime','Hour','Minute','Second','microSecond','Latitude','Longitude','FlightLevel',\n",
    "                             'TargetID', 'SelectedHeading', 'TargetAddress','Direction']]\n",
    "    # Join 2 tables by the \"TargetID\" of point a (for the uniquness)\n",
    "    OrderResult = pd.merge(A.reset_index(drop=True),B.reset_index(drop=True),left_index=True, right_index=True)\n",
    "    # add distance column\n",
    "    OrderResult['Distance'] = d\n",
    "    return OrderResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2beb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the distance of the points closest to each other by longitidue and latitude\n",
    "def proximityCalc(LongitudeOrderDF, LatitudeOrderDF):\n",
    "    longpoint_a = []\n",
    "    longpoint_b = []\n",
    "    longdistance_ab = []\n",
    "\n",
    "    latpoint_a = []\n",
    "    latpoint_b = []\n",
    "    latdistance_ab = []\n",
    "\n",
    "    for a in LongitudeOrderDF.index:\n",
    "        for n in range(1,len(LongitudeOrderDF)):\n",
    "            b = a+n\n",
    "            if b < len(LongitudeOrderDF):\n",
    "                point0 = LongitudeOrderDF.loc[a,'Latitude'], LongitudeOrderDF.loc[a,'Longitude']\n",
    "                pointX = LongitudeOrderDF.loc[b,'Latitude'], LongitudeOrderDF.loc[b,'Longitude']\n",
    "                if pointX[1] <= limit_lon(point0): # Check if longitude of pointX is within the boundary\n",
    "                    distance = distance_d(point0,pointX)\n",
    "                    if distance <= 25: # Check distance within 25 nm\n",
    "                        longpoint_a.append(a)\n",
    "                        longpoint_b.append(b)\n",
    "                        longdistance_ab.append(distance)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "    for a in LatitudeOrderDF.index:   \n",
    "        for n in range(1,len(LatitudeOrderDF)):\n",
    "            b = a+n\n",
    "            if b < len(LatitudeOrderDF):\n",
    "                point0 = LatitudeOrderDF.loc[a,'Latitude'], LatitudeOrderDF.loc[a,'Longitude']\n",
    "                pointX = LatitudeOrderDF.loc[b,'Latitude'], LatitudeOrderDF.loc[b,'Longitude']    \n",
    "                if pointX[0] <= limit_lat(point0):# Check if latitude of pointX is within the boundary\n",
    "                    distance = distance_d(point0,pointX)\n",
    "                    if distance <= 25: # Check distance within 25 nm\n",
    "                        latpoint_a.append(a)\n",
    "                        latpoint_b.append(b)\n",
    "                        latdistance_ab.append(distance)\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    # Apply function to select and merge data frame\n",
    "    LongOrderResult = newDF(LongitudeOrderDF,longpoint_a, longpoint_b,longdistance_ab)\n",
    "    LatOrderResult = newDF(LatitudeOrderDF,latpoint_a, latpoint_b,latdistance_ab)\n",
    "\n",
    "    # Concatenate results from order by longitude and latitude\n",
    "    Resultsdf = pd.concat([LongOrderResult,LatOrderResult]).reset_index(drop=True)\n",
    "\n",
    "    # Delete duplicate pairs of TargetID x and y regardless of order\n",
    "    Resultsdf['list_target'] = Resultsdf.apply(lambda row: tuple(sorted([row['TargetID_x']]+[row['TargetID_y']])), axis = 1)\n",
    "    ResultsDF = Resultsdf.drop_duplicates(subset = 'list_target',keep = 'first').reset_index(drop = True)\n",
    "    ResultsDF.drop('list_target', axis=1, inplace=True)\n",
    "\n",
    "    return (ResultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84f7e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceCalc(resultsDF):\n",
    "    heightDifference = []\n",
    "    potentialLoss1000 = []\n",
    "    potentialLoss400 = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while counter < len(resultsDF):\n",
    "        difference = abs((resultsDF['FlightLevel_x'][counter]) - (resultsDF['FlightLevel_y'][counter]))\n",
    "        heightDifference.append(difference)\n",
    "\n",
    "        if difference <= 1000:\n",
    "            potentialLoss1000.append('True')\n",
    "            if difference <= 400:\n",
    "                potentialLoss400.append('True')\n",
    "            else:\n",
    "                potentialLoss400.append('False')\n",
    "        else:\n",
    "            potentialLoss1000.append('False')\n",
    "            potentialLoss400.append('False')\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "    resultsDF['HeightDifference_ft'] = heightDifference\n",
    "    resultsDF['potentialLoss400'] = potentialLoss400\n",
    "    resultsDF['potentialLoss1000'] = potentialLoss1000\n",
    "\n",
    "    return (resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09d3604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HourCounter = 0\n",
    "finalResults = pd.DataFrame()\n",
    "\n",
    "while HourCounter < 1:\n",
    "    #Create table for the minute\n",
    "    for MinuteCounter in range(0,60):\n",
    "        #Create table for the minute\n",
    "        recordsByMinuteDF = minuteFilter(HourCounter,MinuteCounter)\n",
    "    \n",
    "        #Create longitude and latitude table for proximity analysis\n",
    "        LongitudeOrderDF = lonTableCreate(recordsByMinuteDF)\n",
    "        LatitudeOrderDF = latTableCreate(recordsByMinuteDF)\n",
    "    \n",
    "        #calculate proximity\n",
    "        resultsDF = proximityCalc(LongitudeOrderDF, LatitudeOrderDF)\n",
    "        if resultsDF.empty == True:\n",
    "            # if the results dataframe is empty, then break out of for-loop\n",
    "            break\n",
    "        else:\n",
    "            #Calculate distance\n",
    "            resultsDF = distanceCalc(resultsDF)\n",
    "            #Add the results for this minute to the overall results \n",
    "            finalResults = pd.concat([finalResults, resultsDF], ignore_index=True)\n",
    "    HourCounter = HourCounter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2aea4676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   DateTime_x  Hour_x  Minute_x  Second_x  microSecond_x  \\\n",
      "0  2021-12-24 00:24:02.047000       0        24         2          47000   \n",
      "1  2021-12-24 00:45:00.875000       0        45         0         875000   \n",
      "\n",
      "   Latitude_x  Longitude_x  FlightLevel_x TargetID_x  SelectedHeading_x  ...  \\\n",
      "0   22.278488  -155.472823        29800.0     AAL432          78.046875  ...   \n",
      "1   21.387222  -155.755821        30575.0    SWA1385         196.875000  ...   \n",
      "\n",
      "  Longitude_y FlightLevel_y TargetID_y  SelectedHeading_y  TargetAddress_y  \\\n",
      "0 -155.816895       30000.0     DAL495          45.703125           A789BC   \n",
      "1 -155.441362       30475.0    SWA1310          68.203125           ABFD8C   \n",
      "\n",
      "   Direction_y   Distance  HeightDifference_ft  potentialLoss400  \\\n",
      "0            E  19.153653                200.0              True   \n",
      "1            E  24.140765                100.0              True   \n",
      "\n",
      "   potentialLoss1000  \n",
      "0               True  \n",
      "1               True  \n",
      "\n",
      "[2 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#Get the first entry for this minute of time\n",
    "sql6 = \"SELECT * FROM finalResults WHERE potentialLoss400 = 'True' \"\n",
    "\n",
    "#Run query and store results\n",
    "LossCandidates400 = pq.sqldf(sql6, locals())\n",
    "\n",
    "print(LossCandidates400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46877741",
   "metadata": {},
   "source": [
    "----- NEW CODE FOR TESTING VISUALIZATIONS -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0150d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pandas dataframe to a CSV to export\n",
    "LossCandidates400.to_csv('LossCandidates400.csv', index=False, header=True)\n",
    "\n",
    "#Moving the CSV file of the pandas dataframe into the S3 bucket to download and move to other tools to see what works best\n",
    "import sagemaker, boto3, os\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"demo-sagemaker-filtered-data\"\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'data/LossCandidates400.csv')).upload_file('LossCandidates400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a24081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the points of interest\n",
    "\n",
    "deviation1 = allAircraftData.loc[((allAircraftData['TargetID'] == \"AAL432\") | (allAircraftData['TargetID'] == \"DAL495\")) & ((allAircraftData['Minute'] >= 21) & (allAircraftData['Minute'] <= 27))]\n",
    "\n",
    "deviation2 = allAircraftData.loc[((allAircraftData['TargetID'] == \"SWA1385\") | (allAircraftData['TargetID'] == \"SWA1310\")) & ((allAircraftData['Minute'] >= 42) & (allAircraftData['Minute'] <= 48))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8830b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pandas dataframe to a CSV to export\n",
    "deviation1.to_csv('deviation1.csv', index=False, header=True)\n",
    "\n",
    "#Moving the CSV file of the pandas dataframe into the S3 bucket to download and move to other tools to see what works best\n",
    "import sagemaker, boto3, os\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"demo-sagemaker-filtered-data\"\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'data/deviation1.csv')).upload_file('deviation1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b82d9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert pandas dataframe to a CSV to export\n",
    "deviation2.to_csv('deviation2.csv', index=False, header=True)\n",
    "\n",
    "#Moving the CSV file of the pandas dataframe into the S3 bucket to download and move to other tools to see what works best\n",
    "import sagemaker, boto3, os\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"demo-sagemaker-filtered-data\"\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "    os.path.join(prefix, 'data/deviation2.csv')).upload_file('deviation2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
