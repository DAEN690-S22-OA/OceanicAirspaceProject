{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ed71d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-0.11.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (1.3.4)\n",
      "Collecting shapely<2,>=1.7\n",
      "  Downloading Shapely-1.8.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyproj>=2.6.1.post1\n",
      "  Downloading pyproj-3.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fiona>=1.8\n",
      "  Downloading Fiona-1.8.21-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geopandas) (21.3)\n",
      "Collecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: click>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (59.4.0)\n",
      "Requirement already satisfied: attrs>=17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: six>=1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.0.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.0.0->geopandas) (1.20.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from packaging->geopandas) (3.0.6)\n",
      "Installing collected packages: shapely, pyproj, munch, cligj, click-plugins, fiona, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.21 geopandas-0.11.1 munch-2.5.0 pyproj-3.3.1 shapely-1.8.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting geopy\n",
      "  Downloading geopy-2.2.0-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting geographiclib<2,>=1.49\n",
      "  Downloading geographiclib-1.52-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.52 geopy-2.2.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: shapely in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (1.8.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pandasql\n",
      "  Downloading pandasql-0.7.3.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandasql) (1.20.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandasql) (1.3.4)\n",
      "Requirement already satisfied: sqlalchemy in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandasql) (1.4.27)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas->pandasql) (2021.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from sqlalchemy->pandasql) (1.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.16.0)\n",
      "Building wheels for collected packages: pandasql\n",
      "  Building wheel for pandasql (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26781 sha256=03f7d81a453a1bafb328c112b8b22d75e69102e39e06f75b3894a94c978afac7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ed/8f/46/a383923333728744f01ba24adbd8e364f2cb9470a8b8e5b9ff\n",
      "Successfully built pandasql\n",
      "Installing collected packages: pandasql\n",
      "Successfully installed pandasql-0.7.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Install libraries\n",
    "!pip install geopandas\n",
    "!pip install geopy\n",
    "!pip install shapely \n",
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e537bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import geopandas as ps\n",
    "import geopy as gy\n",
    "import shapely as sy\n",
    "import dask.dataframe as dd\n",
    "import pandasql as pq\n",
    "from shapely.geometry import Point, Polygon\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import sagemaker, boto3, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83bffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the path for the file -- might have to change this if yours is named differently\n",
    "bucket = 'daen-690-pacific-deviations/raw-data' #Bucket name\n",
    "data_key = 'TOMRDate=2021-12-30.csv' #Path to the CSV file \n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "#Import all of the raw data \n",
    "rawData_df = dd.read_csv(data_location, assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c6d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter out the needed attribues, rename, and change flight level scale\n",
    "def filterAttributes():\n",
    "    #New dataframe with selected attributes from the raw data\n",
    "    airspaceData_df = rawData_df[[\"FRN73TMRPDateTimeOfMessageRec\",\"FRN131HRPWCFloatingPointLat\",\"FRN131HRPWCFloatingPointLong\",\n",
    "                     \"FRN145FLFlightLevel\", \"FRN170TITargetId\",\"RESHSelectedHeading\",\"FRN80TATargetAddress\",\n",
    "                     \"FRN161TNTrackNumber\"]]\n",
    "\n",
    "    #Rename columns to make it easier to read\n",
    "    airspaceData_df = airspaceData_df.rename(columns={'FRN73TMRPDateTimeOfMessageRec': 'DateTime', \n",
    "                                                      'FRN131HRPWCFloatingPointLat': \"Latitude\", \n",
    "                                                      'FRN131HRPWCFloatingPointLong': \"Longitude\", \n",
    "                                                      'FRN145FLFlightLevel': \"FlightLevel\", \n",
    "                                                      'FRN170TITargetId': \"TargetID\", \n",
    "                                                      'RESHSelectedHeading': \"SelectedHeading\", \n",
    "                                                      'FRN80TATargetAddress': \"TargetAddress\",\n",
    "                                                      'FRN161TNTrackNumber': \"TrackNumber\"})\n",
    "    \n",
    "    \n",
    "    #Change flight level scale to feet (FL1 = 100 ft)\n",
    "    airspaceData_df['FlightLevel'] = airspaceData_df['FlightLevel'].apply(lambda x: x * 100, meta=('FlightLevel', 'float64'))\n",
    "    \n",
    "    airspaceData = airspaceData_df.compute()\n",
    "    \n",
    "    return airspaceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb1de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to format date and time  \n",
    "def timeFormatting(allAircraftData):\n",
    "    \n",
    "    #Set the dataframe that will be altered through this block of code\n",
    "    #global allAircraftData\n",
    "    \n",
    "    char = ['T','Z']\n",
    "    for x in char:\n",
    "        allAircraftData[\"DateTime\"] = allAircraftData[\"DateTime\"].str.replace( x ,\" \")\n",
    "\n",
    "    # Formatted Datetime\n",
    "    allAircraftData[\"DateTime\"] = pd.to_datetime(allAircraftData[\"DateTime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create 4 new columns for Hour, Minute, Second and Microsecond\n",
    "    allAircraftData[\"Hour\"] = allAircraftData[\"DateTime\"].dt.hour\n",
    "    allAircraftData[\"Minute\"] = allAircraftData[\"DateTime\"].dt.minute\n",
    "    allAircraftData[\"Second\"] = allAircraftData[\"DateTime\"].dt.second\n",
    "    allAircraftData[\"Day\"] = allAircraftData[\"DateTime\"].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Reorder columns\n",
    "    allAircraftData = allAircraftData[[\"DateTime\",\"Day\",\"Hour\",\"Minute\",\"Second\",\"Latitude\",\"Longitude\",\"FlightLevel\",\n",
    "                                   \"TargetID\",\"SelectedHeading\",\"TargetAddress\",\n",
    "                                   \"TrackNumber\"]]\n",
    "    return allAircraftData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25535849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter for only those flights at or above flight level 240\n",
    "def dataFiltering(): \n",
    "    \n",
    "    global airspaceData\n",
    "    global allAircraftData\n",
    "\n",
    "    #Remove anything below FL240\n",
    "    airspaceData = allAircraftData[(allAircraftData['FlightLevel'] >= 24000)]\n",
    "\n",
    "    #Keep only records for the first 5 seconds to speed up processing time \n",
    "    airspaceData = airspaceData[(airspaceData['Second'] < 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b59945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter out anything in the Hawaii airspace\n",
    "def removeHISpace():\n",
    "    \n",
    "    global airspaceData\n",
    "    \n",
    "    #Coordinates for Hawaii airspace\n",
    "    v0 = (26.14472222, -158.62194444) \n",
    "    v1 = (26.105, -160.63166667)\n",
    "    v2 = (25.67611111, -161.69111111)\n",
    "    v3 = (25.05666667, -162.64972222)\n",
    "    v4 = (24.16889, -163.26638889)\n",
    "    v5 = (23.25833, -163.855)\n",
    "    v6 = (22.20555556, -163.91444444)\n",
    "\n",
    "    #Select the correct v7 depending on what you are testing\n",
    "    #v7 = (33.10266389, 130.47177778) #Incorrect point to use during development\n",
    "    v7 = (21.1511111, -163.9144444) #Correct point to use when going live\n",
    "    \n",
    "    v8 = (20.11666667, -163.3)\n",
    "    v9 = (19.65805556,-162.69944444)\n",
    "    v10 = (19.415, -162.38361111)\n",
    "    v11 = (18.40777778, -160.81416667)\n",
    "    v12 = (18.0525, -160.26972222)\n",
    "    v13 = (17.75583333, -159.53888889)\n",
    "    v14 = (17.17055556, -157.75666667) \n",
    "    v15 = (17.805,-156.06805556)\n",
    "    v16 = (18.10888889, -155.71166667)\n",
    "    v17 = (19.14222222, -154.48333333)\n",
    "    v18 = (19.22293333, -151.87963333)\n",
    "    v19 = (20.69694444, -151.01916667) \n",
    "    v20 = (21.54777778, -151.46638889)\n",
    "    v21 = (22.34416667,-151.88527778)\n",
    "    v22 = (23.02416667, -152.57777778)\n",
    "    v23 = (23.78055556, -153.36611111)\n",
    "    v24 = (24.29583333, -154.25)\n",
    "    v25 = (24.72138889, -155.26305556)\n",
    "    v26 = (25.19583333, -156.42111111)\n",
    "\n",
    "    # Polygon\n",
    "    coords = [v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26]\n",
    "    poly = Polygon(coords)\n",
    "    \n",
    "    #Sort flights into what is in the airspace and what is not\n",
    "    hawaiiAir = []\n",
    "\n",
    "    for loc in range(0,len(airspaceData)):\n",
    "        p1 = Point(airspaceData.iloc[loc][5], airspaceData.iloc[loc][6])\n",
    "        hawaiiAir.append(p1.within(poly))\n",
    "\n",
    "    airspaceData['nearHawaii'] = hawaiiAir\n",
    "    \n",
    "    #Filter out only the ones in the airspace\n",
    "    airspaceData = airspaceData[(airspaceData['nearHawaii'] == False)]\n",
    "    airspaceData = airspaceData.drop(columns=['nearHawaii'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6125ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove flights that appear in the record only one time\n",
    "def removeSingleoccurrence():\n",
    "    \n",
    "    global airspaceData\n",
    "    \n",
    "    removeFlights = airspaceData['TargetID'].value_counts().rename_axis('targetID').reset_index(name='counts')\n",
    "\n",
    "    removeFlights = removeFlights[(removeFlights['counts'] == 1)].reset_index(drop = True)\n",
    "\n",
    "    location = 0\n",
    "    for x in removeFlights:\n",
    "        removeID = removeFlights.loc[location][0]\n",
    "        airspaceData = airspaceData[airspaceData.TargetID.str.contains(removeID) == False].reset_index(drop=True)\n",
    "        location = location + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a702c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set the direction of aircraft\n",
    "def aircraftDirection():\n",
    "    \n",
    "    global airspaceData\n",
    "    # Replace missing value with -1\n",
    "    airspaceData['SelectedHeading'] = airspaceData['SelectedHeading'].fillna(-1)\n",
    "    \n",
    "    # Assign Direction \"E\" for 0-180 degree, \"W\" for 180-360 degree, \"NA\" is record with null values \n",
    "    conditionlist = [\n",
    "        (airspaceData['SelectedHeading'] < 0) ,\n",
    "        (airspaceData['SelectedHeading'] >= 0) & (airspaceData['SelectedHeading'] <180),\n",
    "        (airspaceData['SelectedHeading'] > 180)]\n",
    "    choicelist = ['NA', 'E', 'W']\n",
    "    airspaceData['Direction'] = np.select(conditionlist, choicelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1d9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Hour and Minute from 'airspaceData' table\n",
    "def minuteFilter(HourCounter,MinuteCounter):\n",
    "    \n",
    "    global airspaceData\n",
    "\n",
    "    #create SQL query for flights between the start and end time\n",
    "    sql1 = \"SELECT *, min(Second) FROM airspaceData WHERE Hour = '{0}' and Minute = '{1}' GROUP BY TargetID ORDER BY TargetID, Second\".format(HourCounter, MinuteCounter)\n",
    "\n",
    "    #Run query and store results\n",
    "    recordsInMinute = pq.sqldf(sql1, globals())\n",
    "    del recordsInMinute['min(Second)']\n",
    "\n",
    "    return (recordsInMinute.sort_values('Longitude').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db878288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for caluculating distance using 'Haversine formula'\n",
    "def distance_d(point0,pointX):\n",
    "    \n",
    "    # The function \"radians\" is found in the math module\n",
    "    LoA = radians(point0[1])  \n",
    "    LoB = radians(pointX[1])\n",
    "    LaA=  radians(point0[0])  \n",
    "    LaB = radians(pointX[0]) \n",
    "    # The \"Haversine formula\" is used.\n",
    "    D_Lo = LoB - LoA \n",
    "    D_La = LaB - LaA \n",
    "    P = sin(D_La / 2)**2 + cos(LaA) * cos(LaB) * sin(D_Lo / 2)**2  \n",
    "    Q = 2 * asin(sqrt(P))   \n",
    "    \n",
    "    # The earth's radius in kilometers.\n",
    "    R_km = 6371  \n",
    " \n",
    "    # Change the kilometer to  nautical miles\n",
    "    R_nm = R_km*0.539956803\n",
    "\n",
    "    # Then we'll compute the outcome.\n",
    "    return(Q * R_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8e00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to set up boundary at 25 nm by longitude \n",
    "def limit_lon(point0):\n",
    "    \n",
    "    LaA = radians(point0[0])\n",
    "    # calculate distance for one degree of longitude at this latitude(LaA)\n",
    "    onedeg_long = cos(LaA)*(69.172*0.868976242)\n",
    "    add = 25/onedeg_long \n",
    "    pointlimit = (point0[0],point0[1]+add)\n",
    "    \n",
    "    # return Longitude of pointlimit\n",
    "    return pointlimit[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c4ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select, merge and add the values from analyzing Longitude and Latitude\n",
    "def newDF(OrderDF,x,y,d):\n",
    "    \"\"\"DF is Long/LatitudeOrderDF\n",
    "       x = long/latpoint_a\n",
    "       y = long/latpoint_b\n",
    "       d = long/latdistance_ab\"\"\"\n",
    "    # select rows that index is in list 'point_a', 'point_b'\n",
    "    A = OrderDF.loc[x,['DateTime','Day','Hour','Minute','Second','Latitude','Longitude','FlightLevel',\n",
    "                             'TargetID', 'SelectedHeading', 'TargetAddress','Direction']]\n",
    "    B = OrderDF.loc[y,['DateTime','Day','Hour','Minute','Second','Latitude','Longitude','FlightLevel',\n",
    "                             'TargetID', 'SelectedHeading', 'TargetAddress','Direction']]\n",
    "    # Join 2 tables by the \"TargetID\" of point a (for the uniquness)\n",
    "    OrderResult = pd.merge(A.reset_index(drop=True),B.reset_index(drop=True),left_index=True, right_index=True)\n",
    "    # add distance column\n",
    "    OrderResult['Distance'] = d\n",
    "    return OrderResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355bc0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance of the points closest to each other by longitidue and latitude\n",
    "def proximityCalc(LongitudeOrderDF):\n",
    "    longpoint_a = []\n",
    "    longpoint_b = []\n",
    "    longdistance_ab = []\n",
    "\n",
    "    for a in LongitudeOrderDF.index:\n",
    "        for n in range(1,len(LongitudeOrderDF)):\n",
    "            b = a+n\n",
    "            if b < len(LongitudeOrderDF):\n",
    "                point0 = LongitudeOrderDF.loc[a,'Latitude'], LongitudeOrderDF.loc[a,'Longitude']\n",
    "                pointX = LongitudeOrderDF.loc[b,'Latitude'], LongitudeOrderDF.loc[b,'Longitude']\n",
    "                # Check if longitude of pointX is within the boundary\n",
    "                if pointX[1] <= limit_lon(point0): \n",
    "                    distance = distance_d(point0,pointX)\n",
    "                    # Check distance within 25 nm\n",
    "                    if distance <= 25: \n",
    "                        longpoint_a.append(a)\n",
    "                        longpoint_b.append(b)\n",
    "                        longdistance_ab.append(distance)\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    # Apply function to select and merge data frame\n",
    "    Resultsdf = newDF(LongitudeOrderDF,longpoint_a, longpoint_b,longdistance_ab)\n",
    "\n",
    "    return (Resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8d0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating height differences\n",
    "def distanceCalc(resultsDF):\n",
    "    heightDifference = []\n",
    "    potentialLoss1000 = []\n",
    "    potentialLoss400 = []\n",
    "    # Loop through every rows of the input dataframe\n",
    "    for counter in range(0,len(resultsDF)):\n",
    "        difference = abs((resultsDF['FlightLevel_x'][counter]) - (resultsDF['FlightLevel_y'][counter]))\n",
    "        heightDifference.append(difference)\n",
    "        # checkif the difference < 1000\n",
    "        if difference < 1000:\n",
    "            potentialLoss1000.append('True')\n",
    "            # check if the difference < 400\n",
    "            if difference <= 400:\n",
    "                potentialLoss400.append('True')\n",
    "            else:\n",
    "                potentialLoss400.append('False')\n",
    "        else:\n",
    "            potentialLoss1000.append('False')\n",
    "            potentialLoss400.append('False')\n",
    "    # Add columns showing hightdifference(ft), potentialLoss400 and 1000\n",
    "    resultsDF['HeightDifference_ft'] = heightDifference\n",
    "    resultsDF['potentialLoss400'] = potentialLoss400\n",
    "    resultsDF['potentialLoss1000'] = potentialLoss1000\n",
    "\n",
    "    return (resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef330947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing duplicate pairs of aircrafts regardless of order\n",
    "def removeProximityDups(proximityReport):\n",
    "    \n",
    "    proximityReport['TargetID_x'].replace('', np.nan, inplace=True)\n",
    "    proximityReport['TargetID_y'].replace('', np.nan, inplace=True)\n",
    "\n",
    "    proximityReport.dropna(subset=['TargetID_x'], inplace=True)\n",
    "    proximityReport.dropna(subset=['TargetID_y'], inplace=True)\n",
    "    \n",
    "    # sorted targetID in new column 'list_target', then delete the duplicate ones\n",
    "    proximityReport['list_target'] = proximityReport.apply(lambda row: tuple(sorted([row['TargetID_x']]+[row['TargetID_y']])), axis = 1)\n",
    "    proximityReport = proximityReport.drop_duplicates(subset = 'list_target',keep = 'first').reset_index(drop = True)\n",
    "    proximityReport.drop('list_target', axis=1, inplace=True)\n",
    "    \n",
    "    proximityReport = proximityReport[proximityReport.TargetID_x.str.contains(' ') == False].reset_index(drop=True)\n",
    "    proximityReport = proximityReport[proximityReport.TargetID_y.str.contains(' ') == False].reset_index(drop=True)\n",
    "\n",
    "    return proximityReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through every Hour and Minute, and call function to claculate proximity and hight difference\n",
    "def getProximityReport():\n",
    "    proximityReport = pd.DataFrame()\n",
    "\n",
    "    for HourCounter in range(0,24):\n",
    "        #Create table for the minute\n",
    "        for MinuteCounter in range(0,60):\n",
    "            #Create table for the minute\n",
    "            recordsByMinuteDF = minuteFilter(HourCounter,MinuteCounter)\n",
    "\n",
    "            #calculate proximity\n",
    "            resultsDF = proximityCalc(recordsByMinuteDF)\n",
    "\n",
    "            if resultsDF.empty == True:\n",
    "                # if the results dataframe is empty, then break out of for-loop\n",
    "                break\n",
    "            else:\n",
    "                #Calculate distance\n",
    "                resultsDF = distanceCalc(resultsDF)\n",
    "                #Add the results for this minute to the overall results \n",
    "                proximityReport = pd.concat([proximityReport, resultsDF], ignore_index=True)\n",
    "\n",
    "    proximityReport = removeProximityDups(proximityReport)\n",
    "    \n",
    "    return proximityReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbffa134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get only results where potentialLoss at 400ft is True\n",
    "def get400candidate(proximityReport):\n",
    "    LossCandidates400 = proximityReport.loc[(proximityReport['potentialLoss400'] == 'True')]\n",
    "    LossCandidates400 = LossCandidates400.reset_index()\n",
    "    LossCandidates400 = LossCandidates400.drop(columns=['index'])\n",
    "    \n",
    "    if len(LossCandidates400) > 0:\n",
    "        #remove duplicate pairs\n",
    "        LossCandidates400['list_target'] = LossCandidates400.apply(lambda row: tuple(sorted([row['TargetID_x']]+[row['TargetID_y']])), axis = 1)\n",
    "        LossCandidates400 = LossCandidates400.drop_duplicates(subset = ['list_target'],keep = 'last').reset_index(drop = True)\n",
    "        LossCandidates400.drop('list_target', axis=1, inplace=True)\n",
    "\n",
    "    return LossCandidates400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1cb1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get only results where potentialLoss at 1000ft is True\n",
    "def get1000candidate(proximityReport):\n",
    "    LossCandidates1000 = proximityReport.loc[(proximityReport['potentialLoss1000'] == 'True')]\n",
    "    LossCandidates1000 = LossCandidates1000.reset_index()\n",
    "    LossCandidates1000 = LossCandidates1000.drop(columns=['index'])\n",
    "    \n",
    "    if len(LossCandidates1000) > 0:\n",
    "        #remove duplicate pairs\n",
    "        LossCandidates1000['list_target'] = LossCandidates1000.apply(lambda row: tuple(sorted([row['TargetID_x']]+[row['TargetID_y']])), axis = 1)\n",
    "        LossCandidates1000 = LossCandidates1000.drop_duplicates(subset = ['list_target'],keep = 'last').reset_index(drop = True)\n",
    "        LossCandidates1000.drop('list_target', axis=1, inplace=True)\n",
    "\n",
    "    return LossCandidates1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c00f05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the data for the flight at +/- 5 minutes from when the loss of separation was flagged to be under 400 ft\n",
    "def recordsTable(instancesAtLevel, x):\n",
    "\n",
    "    flight_x = instancesAtLevel['TargetID_x'][x]\n",
    "    flight_y = instancesAtLevel['TargetID_y'][x]\n",
    "    \n",
    "    occuranceTime = instancesAtLevel['DateTime_x'][x]\n",
    "    date_format_str = '%Y-%m-%d %H:%M:%S.%f'\n",
    "    occuranceTime = datetime.strptime(occuranceTime, date_format_str)\n",
    "    \n",
    "    #Minutes before and after\n",
    "    n = 5\n",
    "    start_time = occuranceTime - timedelta(minutes=n)\n",
    "    end_time = occuranceTime + timedelta(minutes=n)\n",
    "\n",
    "    flightInformation = allAircraftData.loc[((allAircraftData['TargetID'] == flight_x) | (allAircraftData['TargetID'] == flight_y)) & \n",
    "                                          ((allAircraftData['DateTime'] >= start_time) & (allAircraftData['DateTime'] <= (end_time)))]\n",
    "  \n",
    "    flightInformation = flightInformation.assign(SeparationEntry=x)\n",
    "\n",
    "    return flightInformation.sort_values(by=['SeparationEntry','DateTime', 'TargetID'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c02cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill missing second with linear interpolation \n",
    "def fillSecond(data_x,data_y):\n",
    "    '''This function transform data of target y to be \n",
    "      on the same minute and second as target x'''\n",
    "    # filled with NA in data_y if second_x are not in second_y \n",
    "    Y = data_y.groupby('Minute')['Second'].apply(list).reset_index(name='list')\n",
    "    for i in data_x.index:\n",
    "        min_x = data_x.loc[i,'Minute']\n",
    "        sec_x = data_x.loc[i,'Second']\n",
    "        for n in range(0,len(Y)):\n",
    "            min_y = Y.loc[n,'Minute']\n",
    "            if min_x == min_y:\n",
    "                listsec = Y.loc[n,'list']\n",
    "                if (sec_x not in listsec):\n",
    "                    ydict = {'Minute': min_x, 'Second': sec_x, \n",
    "                   'TargetID': data_y.loc[0,'TargetID']}\n",
    "                    data_y = data_y.append(ydict, ignore_index = True)\n",
    "\n",
    "    # fill NA with linear interpolation method\n",
    "    y_interp = data_y.sort_values(by=['Minute','Second']).interpolate(method='linear', limit_direction ='forward')\n",
    "    y_transformed = y_interp.interpolate(method='linear', limit_direction ='backward')\n",
    "    return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85549eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to put data_x and data_y on the same time scale\n",
    "def transformTable(flightData):\n",
    "    for i, id in enumerate(flightData['TargetID'].unique()):\n",
    "        if i == 0:\n",
    "            data_x = flightData[(flightData['TargetID']== id)].reset_index(drop = True)\n",
    "        else:\n",
    "            data_y = flightData[(flightData['TargetID']== id)].reset_index(drop = True)\n",
    "\n",
    "    data_x = data_x[['SeparationEntry','DateTime','Day','Hour','Minute','Second','Latitude','Longitude','FlightLevel','TargetID','SelectedHeading']]\n",
    "    data_y = data_y[['Minute','Second','Latitude','Longitude','FlightLevel','TargetID','SelectedHeading']]\n",
    "    \n",
    "    y_transformed = fillSecond(data_x,data_y)\n",
    "\n",
    "    analyzedTable = pd.merge(data_x,y_transformed,on=['Minute','Second'], how='left')\n",
    "\n",
    "    analyzedTable['Latitude_y'].replace('', np.nan, inplace=True)\n",
    "    analyzedTable['Latitude_y'].replace('', np.nan, inplace=True)\n",
    "\n",
    "    analyzedTable.dropna(subset=['Latitude_y'], inplace=True)\n",
    "    analyzedTable.dropna(subset=['Latitude_y'], inplace=True)\n",
    "    \n",
    "    return analyzedTable.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef31d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance using 'Haversine formula'\n",
    "def haversineAnalysis(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "    a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "\n",
    "    return earth_radius * 2 * np.arcsin(np.sqrt(a))  * 0.539956803 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91c687ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and append 'LateralDistance' column\n",
    "def getLateralDist(analyzedTable):\n",
    "\n",
    "    analyzedTable['LateralDistance'] = \\\n",
    "    haversineAnalysis(analyzedTable.Latitude_x, analyzedTable.Longitude_x,\n",
    "                 analyzedTable.Latitude_y, analyzedTable.Longitude_y)\n",
    "  \n",
    "    return analyzedTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "644dfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and append the Flight Level differnece column \n",
    "def flightlevelCalc(analyzedTable):\n",
    "\n",
    "    flightlevelDifference = []\n",
    "\n",
    "    for counter in range(0,len(analyzedTable)):\n",
    "        Diff = abs((analyzedTable['FlightLevel_x'][counter]) - (analyzedTable['FlightLevel_y'][counter]))\n",
    "        flightlevelDifference.append(Diff)\n",
    "        \n",
    "    analyzedTable['FlightLevelDifference'] = flightlevelDifference\n",
    "\n",
    "    return analyzedTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2222a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify direction of aircraft\n",
    "def getDirection(analyzedTable):\n",
    "\n",
    "    # Direction\n",
    "    conditionsX = [(analyzedTable.iloc[-2]['Longitude_x'] - analyzedTable.iloc[0]['Longitude_x'] < 0),(analyzedTable.iloc[-2]['Longitude_x'] - analyzedTable.iloc[0]['Longitude_x'] > 0)]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['W', 'E']\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    analyzedTable['X_direction'] = np.select(conditionsX, values)\n",
    "\n",
    "    conditionsY = [\n",
    "      (analyzedTable.iloc[-2]['Longitude_y'] - analyzedTable.iloc[0]['Longitude_y'] < 0),\n",
    "      (analyzedTable.iloc[-2]['Longitude_y'] - analyzedTable.iloc[0]['Longitude_y'] > 0)\n",
    "      ]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['W', 'E']\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    analyzedTable['Y_direction'] = np.select(conditionsY, values)\n",
    "\n",
    "    analyzedTable = analyzedTable[analyzedTable.columns[[0,1,2,3,4,5,6,7,8,9,10,18,11,12,13,14,15,19,16,17]]]\n",
    "\n",
    "    return analyzedTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ba25ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to get separation report contains information at loss of separation and 5 minutes before and after\n",
    "def getSeaparationReports(instancesAtLevel):\n",
    "    \n",
    "    global analysis #REMOVE AFTER TESTING\n",
    "    separationReport = pd.DataFrame()\n",
    "\n",
    "    for x in range(0,len(instancesAtLevel.index)):\n",
    "        #Get the data for the flight at +/- 5 minutes \n",
    "        flightData = recordsTable(instancesAtLevel, x)\n",
    "\n",
    "        #Format the table for output\n",
    "        analyzedTable = transformTable(flightData)\n",
    "\n",
    "        #Compute/assign lateral separation, height separation, and direction\n",
    "        analyzedTable = getLateralDist(analyzedTable)\n",
    "        analyzedTable = flightlevelCalc(analyzedTable)\n",
    "        analyzedTable = getDirection(analyzedTable)\n",
    "\n",
    "        #Add table to the results \n",
    "        separationReport = pd.concat([separationReport, analyzedTable], ignore_index=True)\n",
    "\n",
    "    return separationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44b377ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get information of only TargetID x\n",
    "def flightXInfo(separationData):\n",
    "\n",
    "    flightX = []\n",
    "\n",
    "    for x in range(0, len(separationData.index)):\n",
    "        values_x = [separationData['SeparationEntry'].loc[x], \n",
    "                separationData['DateTime'].loc[x], \n",
    "                separationData['FlightLevel_x'].loc[x], \n",
    "                separationData['TargetID_x'].loc[x], \n",
    "                separationData['X_direction'].loc[x], \n",
    "                separationData['LateralDistance'].loc[x], \n",
    "                separationData['FlightLevelDifference'].loc[x], \n",
    "                separationData['Longitude_x'].loc[x], \n",
    "                separationData['Latitude_x'].loc[x]]\n",
    "        flightX.append(values_x)\n",
    "\n",
    "    return flightX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1408c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get information of only TargetID y\n",
    "def flightYInfo(separationData):\n",
    "\n",
    "    flightY = []\n",
    "\n",
    "    for x in range(0, len(separationData.index)):\n",
    "        values_y = [separationData['SeparationEntry'].loc[x],\n",
    "                separationData['DateTime'].loc[x],\n",
    "                separationData['FlightLevel_y'].loc[x], \n",
    "                separationData['TargetID_y'].loc[x], \n",
    "                separationData['Y_direction'].loc[x], \n",
    "                separationData['LateralDistance'].loc[x], \n",
    "                separationData['FlightLevelDifference'].loc[x],\n",
    "                separationData['Longitude_y'].loc[x],\n",
    "                separationData['Latitude_y'].loc[x]]\n",
    "        flightY.append(values_y)\n",
    "\n",
    "    return flightY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de030616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create table for visualization\n",
    "def getVisTable(Resulttable):\n",
    "    \n",
    "    xvalues = pd.DataFrame(flightXInfo(Resulttable))\n",
    "    yvalues = pd.DataFrame(flightYInfo(Resulttable))\n",
    "    # concatenate by index\n",
    "    tableToVisualize = pd.concat([xvalues, yvalues], ignore_index=True)\n",
    "    tableToVisualize = tableToVisualize.rename(columns={0: 'SeparationEntry',\n",
    "                                                      1: 'DateTime', \n",
    "                                                      2: \"FlightLevel\", \n",
    "                                                      3: \"TargetID\", \n",
    "                                                      4: \"Direction\", \n",
    "                                                      5: \"LateralDistance\",\n",
    "                                                      6: \"FLDifference\",\n",
    "                                                      7: \"Longitude\", \n",
    "                                                      8: \"Latitude\"})\n",
    "\n",
    "    return tableToVisualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7334b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the report at the 1000 level\n",
    "def getSeparation1000Report(proximityReport):\n",
    "\n",
    "    flSeparation1000Report = get1000candidate(proximityReport)    \n",
    "\n",
    "    if len(flSeparation1000Report) > 0:\n",
    "        flSeparation1000Report = getSeaparationReports(flSeparation1000Report)\n",
    "    \n",
    "    flSeparation1000Report['TargetID_x'].replace('', np.nan, inplace=True)\n",
    "    flSeparation1000Report['TargetID_y'].replace('', np.nan, inplace=True)\n",
    "\n",
    "    flSeparation1000Report.dropna(subset=['TargetID_x'], inplace=True)\n",
    "    flSeparation1000Report.dropna(subset=['TargetID_y'], inplace=True)\n",
    "    \n",
    "    return flSeparation1000Report.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f92e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the visualization data at the 1000 level\n",
    "def visualization1000(report1000): \n",
    "    \n",
    "    if len(report1000) > 0:\n",
    "        viz1000Data = getVisTable(report1000)\n",
    "        conditions = [\n",
    "            (viz1000Data['FLDifference'] < 1000) & (viz1000Data['LateralDistance'] <= 25),\n",
    "            (viz1000Data['FLDifference'] < 1000) & (viz1000Data['LateralDistance'] > 25),\n",
    "            (viz1000Data['FLDifference'] >= 1000) & (viz1000Data['LateralDistance'] <= 25),\n",
    "            (viz1000Data['FLDifference'] >= 1000) & (viz1000Data['LateralDistance'] > 25),\n",
    "        ]\n",
    "        values = ['True','False','False','False']\n",
    "        viz1000Data['potentialLoss'] = np.select(conditions, values)\n",
    "    else:\n",
    "        column_names = ['SeparationEntry', 'DateTime','FlightLevel','TargetID','Direction','LateralDistance',\n",
    "                       'FLDifference', 'Longitude', 'Latitude']\n",
    "        viz1000Data = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    return viz1000Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e45f2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the report at the 400 level\n",
    "def getSeparation400Report(proximityReport):\n",
    "    \n",
    "    #Get the potential separations at 400\n",
    "    flSeparation400Report = get400candidate(proximityReport)\n",
    "\n",
    "    #Get the flight details if there were any potentail separations at the 400 level\n",
    "    if len(flSeparation400Report) > 0:\n",
    "        flSeparation400Report = getSeaparationReports(flSeparation400Report)\n",
    "        \n",
    "        \n",
    "    #Remove missing data due to there not being recordings for the time frame around the potential separation\n",
    "    flSeparation400Report['TargetID_x'].replace('', np.nan, inplace=True)\n",
    "    flSeparation400Report['TargetID_y'].replace('', np.nan, inplace=True)\n",
    "\n",
    "    flSeparation400Report.dropna(subset=['TargetID_x'], inplace=True)\n",
    "    flSeparation400Report.dropna(subset=['TargetID_y'], inplace=True)\n",
    "    \n",
    "    return flSeparation400Report.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c493748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the visualization data at the 400 level\n",
    "def visualization400(report400):\n",
    "    \n",
    "    if len(report400) > 0:\n",
    "        viz400Data = getVisTable(report400)\n",
    "        conditions = [\n",
    "            (viz400Data['FLDifference'] < 400) & (viz400Data['LateralDistance'] <= 25),\n",
    "            (viz400Data['FLDifference'] < 400) & (viz400Data['LateralDistance'] > 25),\n",
    "            (viz400Data['FLDifference'] >= 400) & (viz400Data['LateralDistance'] <= 25),\n",
    "            (viz400Data['FLDifference'] >= 400) & (viz400Data['LateralDistance'] > 25),\n",
    "        ]\n",
    "        values = ['True','False', 'False','False']\n",
    "        viz400Data['potentialLoss'] = np.select(conditions, values)\n",
    "    else:\n",
    "        column_names = ['SeparationEntry', 'DateTime','FlightLevel','TargetID','Direction','LateralDistance',\n",
    "                       'FLDifference', 'Longitude', 'Latitude']\n",
    "        viz400Data = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    return viz400Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cd3ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export all report files into S3 bucket\n",
    "def exportFiles(proximityReport, separation400Report, visualization400Report, separation1000Report, visualization1000Report):\n",
    "    \n",
    "    reportDate = proximityReport['Day_x'][0]\n",
    "    \n",
    "    #Create CSV file\n",
    "    proximityReport.to_csv('proximityReport.csv', index=False, header=True)\n",
    "    separation400Report.to_csv('separation400Report.csv', index=False, header=True)\n",
    "    visualization400Report.to_csv('visualization400Report.csv', index=False, header=True)\n",
    "    separation1000Report.to_csv('separation1000Report.csv', index=False, header=True)\n",
    "    visualization1000Report.to_csv('visualization1000Report.csv', index=False, header=True)\n",
    "    \n",
    "    bucket = sagemaker.Session().default_bucket()\n",
    "    prefix = \"potential-loss-separation-{0}\".format(reportDate) #Rename for final solution\n",
    "\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "        os.path.join(prefix, 'data/proximityReport.csv')).upload_file('proximityReport.csv')\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "        os.path.join(prefix, 'data/separation400Report.csv')).upload_file('separation400Report.csv')\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "        os.path.join(prefix, 'data/visualization400Report.csv')).upload_file('visualization400Report.csv')\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "        os.path.join(prefix, 'data/separation1000Report.csv')).upload_file('separation1000Report.csv')\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(\n",
    "        os.path.join(prefix, 'data/visualization1000Report.csv')).upload_file('visualization1000Report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ab211e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call data cleaning steps\n",
    "def cleanAllAircraftData():\n",
    "    \n",
    "    global allAircraftData\n",
    "    \n",
    "    allAircraftData = filterAttributes()\n",
    "    allAircraftData = timeFormatting(allAircraftData)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed9383eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call data cleaning steps for analysis\n",
    "def cleanAirspaceData():\n",
    "    \n",
    "    airspaceData = dataFiltering()\n",
    "    airspaceData = removeHISpace()\n",
    "    airspaceData = removeSingleoccurrence()\n",
    "    airspaceData = aircraftDirection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "808eee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3215.695829629898 seconds ---\n"
     ]
    }
   ],
   "source": [
    "airspaceData = pd.DataFrame()\n",
    "allAircraftData = pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    global allAircraftData\n",
    "    global airspaceData   \n",
    "\n",
    "    #Functions to call for the data cleanup\n",
    "    cleanAllAircraftData()\n",
    "    cleanAirspaceData()\n",
    "\n",
    "    #Functions to call for the reports\n",
    "    proximityReport = getProximityReport()\n",
    "    separation400Report = getSeparation400Report(proximityReport)\n",
    "    visualization400Report = visualization400(separation400Report)\n",
    "    separation1000Report = getSeparation1000Report(proximityReport)\n",
    "    visualization1000Report = visualization1000(separation1000Report)\n",
    "\n",
    "    #Export and Save reports\n",
    "    exportFiles(proximityReport, separation400Report, visualization400Report, separation1000Report, visualization1000Report)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
